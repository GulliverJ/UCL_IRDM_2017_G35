{'html': b'<!DOCTYPE html>\r\n\r\n<!-- HTML5 Mobile Boilerplate -->\r\n<!--[i'
         b'f IEMobile 7]><html class="no-js iem7"><![endif]-->\r\n<!--[if (gt'
         b' IEMobile 7)|!(IEMobile)]><!--><html class="no-js" lang="en"><!--<!['
         b'endif]-->\r\n\r\n<!-- HTML5 Boilerplate -->\r\n<!--[if lt IE 7]><h'
         b'tml class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->\r\n'
         b'<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="'
         b'en"><![endif]-->\r\n<!--[if (IE 8)&!(IEMobile)]><html class="no-js'
         b' lt-ie9" lang="en"><![endif]-->\r\n<!--[if gt IE 8]><!--> <html cl'
         b'ass="no-js" lang="en"><!--<![endif]-->\r\n\r\n<head>\r\n\r\n\t<me'
         b'ta charset="utf-8">\r\n\t<!-- Always force latest IE rendering engi'
         b'ne (even in intranet) & Chrome Frame -->\r\n\t<meta http-equiv="X-U'
         b'A-Compatible" content="IE=edge,chrome=1">\r\n\r\n\t<title>Dr. Fab'
         b'rizio Pece: Publications</title>\r\n\t<meta name="description" cont'
         b'ent="Fabrizio Pece personal space.">\r\n\t<meta name="keywords" con'
         b'tent="fabrizio pece, UCL, CS, computer science, computer vision, com'
         b'puter graphics, virtual environments">\r\n\t<link rel="author" href'
         b'="/humans.txt">\r\n\r\n\t<meta http-equiv="cleartype" content="on'
         b'">\r\n\r\n\t<link rel="shortcut icon" href="./../favicon.ico">\r\n\r'
         b'\n\t<!-- Responsive and mobile friendly stuff -->\r\n\t<meta name'
         b'="HandheldFriendly" content="True">\r\n\t<meta name="MobileOptimize'
         b'd" content="320">\r\n\t<meta name="viewport" content="width=device-'
         b'width, initial-scale=1.0">\r\n\t\r\n\t<!-- Stylesheets -->\r\n\t<'
         b'link rel="stylesheet" href="./../css/html5reset.css" media="all"'
         b'>\r\n\t<link rel="stylesheet" href="./../css/fabriziopece.css" medi'
         b'a="all">\r\n\t<link rel="stylesheet" href="./../css/tada.css" media'
         b'="all">\r\n\r\n\t<!-- Responsive Stylesheets -->\r\n\t<link rel="sty'
         b'lesheet" media="only screen and (max-width: 1024px) and (min-width: '
         b'769px)" href="./../css/1024.css">\r\n\t<link rel="stylesheet" media'
         b'="only screen and (max-width: 768px) and (min-width: 481px)" href=".'
         b'/../css/768.css">\r\n\t<link rel="stylesheet" media="only screen an'
         b'd (max-width: 480px)" href="/css/480.css">\r\n\r\n\t<!-- Google A'
         b'nalytics Scripts-->\r\n\t<script>\r\n\t  (function(i,s,o,g,r,a,m){'
         b"i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\r\n\t  (i[r].q="
         b'i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o'
         b'),\r\n\t  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parent'
         b"Node.insertBefore(a,m)\r\n\t  })(window,document,'script','//www.go"
         b"ogle-analytics.com/analytics.js','ga');\r\n\r\n\t  ga('create', '"
         b"UA-45679053-1', 'ucl.ac.uk');\r\n\t  ga('send', 'pageview');\r\n\t"
         b'</script>\r\n\r\n\t<!-- All JavaScript at the bottom, except for '
         b'Modernizr which enables HTML5 elements and feature detects -->\r\n'
         b'\t<script src="/js/modernizr-2.5.3-min.js"></script>\r\n\t<script ty'
         b'pe="text/javascript">\r\n\r\n\t  var _gaq = _gaq || [];\r\n\t  _gaq.'
         b"push(['_setAccount', 'UA-31237166-1']);\r\n\t  _gaq.push(['_trackPa"
         b"geview']);\r\n\r\n\t  (function() {\r\n\t\tvar ga = document.crea"
         b"teElement('script'); ga.type = 'text/javascript'; ga.async = tru"
         b"e;\r\n\t\tga.src = ('https:' == document.location.protocol ? 'https:"
         b"//ssl' : 'http://www') + '.google-analytics.com/ga.js';\r\n\t\tvar s"
         b" = document.getElementsByTagName('script')[0]; s.parentNode.insertBe"
         b'fore(ga, s);\r\n\t  })();\r\n\r\n\tfunction group(by)\r\n\t{\r\n\t'
         b"\t$('h2 span.group').unbind();\r\n\r\n\t\tvar years = [2013, 2012, "
         b"2011, 2010, 2009, 2008, 2007];\r\n\t\tvar types = ['journal', 'confe"
         b"rence', 'workshop', 'other', 'thesis'];\r\n\t\tvar labels = {\r\n\t"
         b"\t\tjournal: 'Journal Articles',\r\n\t\t\tconference: 'Conference P"
         b"apers',\r\n\t\t\tworkshop: 'Workshop Contributions',\r\n\t\t\tothe"
         b"r: 'Other Contributions',\r\n\t\t\tthesis: 'Theses'\r\n\t\t};"
         b'\r\n\r\n\t\tif ($("div.papers_old").length==0) $("div.papers").add'
         b'Class(\'papers_old\').removeClass("papers");\r\n\t\telse $("div.pa'
         b'pers").remove();\r\n\r\n\t\tif (by==\'year\')\r\n\t\t{\r\n\t\t\tv'
         b'ar string = "<div class=\'papers\'>"\r\n\t\t\tfor (var i=0; i < yea'
         b'rs.length; i++)\r\n\t\t\t{\r\n\t\t\t\tstring += "<h3>"+years[i]+"</'
         b'h3><ul class=\'papers "+years[i]+"\'></ul>";\r\n\t\t\t}\r\n\t\t\tstr'
         b'ing += "</div>";\r\n\r\n\t\t\t$("div.papers_old").before(string'
         b');\r\n\t\t}\r\n\t\telse\r\n\t\t{\r\n\t\t\tvar string = "<div clas'
         b's=\'papers\'>"\r\n\t\t\tfor (var i=0; i < types.length; i++)\r\n\t'
         b'\t\t{\r\n\t\t\t\tstring += "<h3>"+labels[types[i]]+"</h3><ul cla'
         b'ss=\'papers "+types[i]+"\'></ul>";\r\n\t\t\t}\r\n\t\t\tstring += "</'
         b'div>";\r\n\r\n\t\t\t$("div.papers_old").before(string);\r\n\t\t'
         b'}\r\n\r\n\t\t$("div.papers_old li").each(function(index, el) {'
         b'\r\n\r\n\t\t\tvar classes = $(el).attr(\'class\').split(" ");'
         b'\r\n\r\n\t\t\tvar y = classes[0];\r\n\t\t\tvar t = classes[1];\r\n'
         b'\r\n\t\t\tif (by==\'year\') $("div.papers ul."+y).append($(el).clon'
         b'e());\r\n\t\t\telse $("div.papers ul."+t).append($(el).clone());\r'
         b'\n\t\t});\r\n\r\n\t\t$("div.papers_old").css({display: \'none\'});\r'
         b"\n\r\n\t\tif (by=='year') $('h2 span.group').click(function(){gro"
         b"up('type')}).html('by type');\r\n\t\telse $('h2 span.group').click(f"
         b"unction(){group('year')}).html('by year');\r\n\r\n\r\n\r\n\t}"
         b'\r\n\t\r\n\tfunction hiddenemail(d,u,t)\r\n\t{\r\n\t   var email = u'
         b' + "@" + d;\r\n\t   if (t)\r\n\t      document.write("<a href=mail'
         b'to:"+email+">"+t+"</a>");\r\n\t   else\r\n\t      document.write("'
         b'<a href=mailto:"+email+">"+email+"</a>");\r\n\t}\r\n\t\r\n\t</scr'
         b'ipt>\r\n\t\r\n\t<link rel="canonical" href="http://www.cs.ucl.ac.u'
         b'k/staff/F.Pece/"/>\r\n\t<meta name="robots" content="index, follow"'
         b' />\r\n\r\n\t<link href="http://vjs.zencdn.net/c/video-js.css" re'
         b'l="stylesheet">\r\n\t<script src="http://vjs.zencdn.net/c/video.js"'
         b'></script>\r\n\r\n</head>\r\n\r\n<!-- <body onload="group(\'year\''
         b')"> -->\r\n\t<body>\r\n\r\n<div id="skiptomain"><a href="#mainconte'
         b'nt">skip to main content</a></div>\r\n\r\n<div id="wrapper">\r\n\t<'
         b'div id="headcontainer">\r\n\t\t<header class="group">\r\n\t\t\t<!'
         b'-- <div id="toplogo"><a href="./../index.html"><img src="./../images'
         b'/grahamrobertsonmiller.png" width="105" height="100" alt="graham rob'
         b'ertson miller logo: click for home page" class="tada" /></a><p><a hr'
         b'ef="/">Fabrizio Pece</a></p><span>PhD Student</span></div> -->\r\n'
         b'\t\t\t\t<div id="toplogo"><br><p><a href="./../index.html"><h1>Dr. F'
         b'abrizio Pece</h1></a></p><span>Postdoctoral Researcher</span></d'
         b'iv>\r\n\r\n\t\t\t<nav>\r\n\r\n\t\t\t\t<ul id="menu">\r\n\t\t\t\t\t'
         b'\t<br><br><br>\r\n\t\t\t\t\t\t<!-- <li class="nav1"><a href="./..'
         b'/research/index.html">Research</a></li> -->\r\n\r\n\t\t\t\t\t'
         b'\t\r\n\r\n\t\t\t\t\t\t<li class="nav2"><a href="./../publications/i'
         b'ndex.html">Publications</a></li>\r\n\r\n\t\t\t\t\t\t\r\n\r\n\t\t'
         b'\t\t\t\t<!-- <li class="nav3"><a href="./../cv/index.html">CV</a></l'
         b'i> -->\t\t\t\t\t\r\n\r\n\t\t\t\t</ul>\r\n\r\n\t\t\t</nav>\r\n\r'
         b'\n\r\n\t\t</header>\r\n\t</div>\r\n\t\r\n\t<div id="papercontainer'
         b'">\r\n\t\t<div id="papercontent"  class="group">\r\n\t\t\t<div cl'
         b'ass="section group">\r\n\t\t\t\t<h2>Publications <span class=\'grou'
         b'p\'></span></h2>\t\r\n\t\t\t\t<div class="papers">\r\n\t\t\t\t\t<h3'
         b'>2015</h3>\r\n\t\t\t\t\t\r\n\t\t\t\t\t<li class="2015 paper">\r'
         b'\n\t\t\t\t\t<em>Joint Estimation of 3D Hand Position and Gestures '
         b'from Monocular Video for Mobile Interaction</em>\r\n\t\t\t\t\t&nbsp'
         b';&nbsp;<a href="http://ait.inf.ethz.ch/projects/2015/3Dfrom2D/" clas'
         b's="button">Project Page</a><br>\r\n\t\t\t\t\tJ. Song, F. Pece, G. S'
         b'\xc3\xb6r\xc3\xb6s, M. Koelle, O. Hilliges<br>\r\n\t\t\t\t\t<b>ACM '
         b'Human Factors in Computing Systems (CHI 2015)</b> <br>\r\n\t\t\t\t'
         b'\tSeoul, South Korea, Apr. 2015. <br>\r\n\t\t\t\t\t<!-- <em><a href='
         b'"http://ait.inf.ethz.ch/projects/2014/InAirGesture/downloads/song201'
         b'4.bib">Bibtex</a></em><br> -->\r\n\t\t\t\t\t<!--<em><a href="http:/'
         b'/ait.inf.ethz.ch/projects/2014/InAirGesture/downloads/InairGesturesA'
         b'roundUnmodifiedMobileDevice_UIST_2014_Song.mp4">Video</a></em><br> -'
         b'->\r\n\t\t\t\t\t<em><a class="htooltip" href="#">Abstract\r\n'
         b'\t\t\t\t\t<span>\r\n\t\t\t\t\t\tWe present a machine learning tec'
         b'hnique to recognize gestures and estimate metric depth of hands for '
         b'3D interaction, relying only on monocular RGB video input. We aim to'
         b' enable spatial interaction with small, body-worn devices where rich'
         b' 3D input is desired but the usage of conventional depth sensors is '
         b'prohibitive due to their power consumption and size. We propose a hy'
         b'brid classification-regression approach to learn and predict a mappi'
         b'ng of RGB colors to absolute, metric depth in real time. We also cla'
         b'ssify distinct hand gestures, allowing for a variety of 3D interacti'
         b'ons. We demonstrate our technique with three mobile interaction scen'
         b'arios and evaluate the method quantitatively and qualitatively.\r'
         b'\n\t\t\t\t\t</span>\r\n\t\t\t\t\t</a></em>\r\n\t\t\t\t\t</li>\r\n\t'
         b'\t\t\t\t<br>\r\n\t\t\t\t\t\r\n\t\t\t\t\t<h3>2014</h3>\r\n\t\t\t'
         b'\t\t\r\n\t\t\t\t\t<li class="2014 paper">\r\n\t\t\t\t\t<em><a href="'
         b'http://ait.inf.ethz.ch/projects/2014/InAirGesture/downloads/p319-son'
         b'g.pdf" class="help_link">In-air Gestures Around Unmodified Mobile De'
         b'vices</a></em>\r\n\t\t\t\t\t&nbsp;&nbsp;<a href="http://ait.inf.eth'
         b'z.ch/projects/2014/InAirGesture/" class="button">Project Page</a><br'
         b'>\r\n\t\t\t\t\tJ. Song, G. S\xc3\xb6r\xc3\xb6s, F. Pece, S. Fanello'
         b', S. Izadi, C. Keskin, O. Hilliges<br>\r\n\t\t\t\t\t<b>Symposium on'
         b' User Interface Software and Technology (UIST 2014)</b> <br>\r\n\t\t'
         b'\t\t\tHonolulu, Hawaii, Oct. 6-8, 2014. <br>\r\n\t\t\t\t\t<em><a h'
         b'ref="http://ait.inf.ethz.ch/projects/2014/InAirGesture/downloads/son'
         b'g2014.bib">Bibtex</a></em><br>\r\n\t\t\t\t\t<em><a href="http://ait'
         b'.inf.ethz.ch/projects/2014/InAirGesture/downloads/InairGesturesAroun'
         b'dUnmodifiedMobileDevice_UIST_2014_Song.mp4">Video</a></em><br>\r\n'
         b'\t\t\t\t\t<em><a class="htooltip" href="#">Abstract\r\n\t\t\t\t\t<sp'
         b'an>\r\n\t\t\t\t\t\tWe present a novel machine learning based algorit'
         b'hm ex- tending the interaction space around mobile devices. The tech'
         b'nique uses only the RGB camera now commonplace on off-the-shelf mobi'
         b'le devices. Our algorithm robustly recog- nizes a wide range of in-a'
         b'ir gestures, supporting user varia- tion, and varying lighting condi'
         b'tions. We demonstrate that our algorithm runs in real-time on unmodi'
         b'fied mobile devices, in- cluding resource-constrained smartphones an'
         b'd smartwatches. Our goal is not to replace the touchscreen as primar'
         b'y input device, but rather to augment and enrich the existing intera'
         b'c- tion vocabulary using gestures. While touch input works well for '
         b'many scenarios, we demonstrate numerous interaction tasks such as mo'
         b'de switches, application and task manage- ment, menu selection and c'
         b'ertain types of navigation, where such input can be either complemen'
         b'ted or better served by in- air gestures. This removes screen real-e'
         b'state issues on small touchscreens, and allows input to be expanded '
         b'to the 3D space around the device. We present results for recognitio'
         b'n accuracy (93% test and 98% train), impact of memory footprint and '
         b'other model parameters. Finally, we report results from pre- liminar'
         b'y user evaluations, discuss advantages and limitations and conclude '
         b'with directions for future work.\r\n\t\t\t\t\t</span>\r\n\t\t\t\t'
         b'\t</a></em>\r\n\t\t\t\t\t</li>\r\n\t\t\t\t\t<br>\r\n\t\t\t\t\t'
         b'\r\n\t\t\t\t\t<li class="2014 paper">\r\n\t\t\t\t\t<em><a href="./'
         b'pdfs/CVMP2014_0021_FinalVersion.pdf" class="help_link">Device Effect'
         b' on Panoramic Video+Context Tasks</a></em><br>\r\n\t\t\t\t\tF. Pece'
         b', J. Tompkin, H.P. Pfister, J. Kautz, C. Theobalt<br>\r\n\t\t\t\t\t'
         b'<b>Conference on Visual Media Production (CVMP 2014)</b><br>\r\n\t\t'
         b'\t\t\tLondon, UK, 13-14 November 2014<br>\r\n\t\t\t\t\t<em><a href'
         b'="./videos/CVMP2014_0021_FinalVideo.mp4">Video</a></em><br>\r'
         b'\n\t\t\t\t\t<em><a class="htooltip" href="#">Abstract\r\n\t\t\t'
         b'\t\t<span>\r\n\t\t\t\t\t\tPanoramic imagery is viewed daily by tho'
         b'usands of people, and\r\n\t\t\t\t\t\tpanoramic video imagery is beco'
         b'ming more common. This imagery\r\n\t\t\t\t\t\tis viewed on many diff'
         b'erent devices with different properties, and\r\n\t\t\t\t\t\tthe effe'
         b'ct of these differences on spatio-temporal task performance\r'
         b'\n\t\t\t\t\t\tis yet untested on these imagery. We adapt a novel pa'
         b'noramic video\r\n\t\t\t\t\t\tinterface and conduct a user study to d'
         b'iscover whether display type\r\n\t\t\t\t\t\taffects spatio-temporal '
         b'reasoning task performance across desktop\r\n\t\t\t\t\t\tmonitor, ta'
         b'blet, and head-mounted displays. We discover that, in our\r\n\t'
         b'\t\t\t\t\tcomplex reasoning task, HMDs are as effective as deskto'
         b'p displays\r\n\t\t\t\t\t\teven if participants felt less capable, bu'
         b't tablets were less effective\r\n\t\t\t\t\t\tthan desktop displays e'
         b'ven though participants felt just as capable.\r\n\t\t\t\t\t\tOur res'
         b'ults impact virtual tourism, telepresence, and surveillance\r'
         b'\n\t\t\t\t\t\tapplications, and so we state the design implications'
         b' of our results\r\n\t\t\t\t\t\tfor panoramic imagery systems.\r\n\t'
         b'\t\t\t\t</span>\r\n\t\t\t\t\t</a></em>\r\n\t\t\t\t\t</li>\r\n\t\t\t'
         b'\t\t<br>\r\n\t\t\t\t\t\r\n\t\t\t\t\t<h3>2013</h3>\r\n\r\n\t\t\t'
         b'\t\t<ul class="papers">\r\n\r\n\t\t\t\t\t<li class="2013 paper"'
         b'>\r\n\t\t\t\t\t<em><a href="./pdfs/vidicontexts.pdf" class="help_li'
         b'nk">Video Collections in Panoramic Contexts</a></em>\r\n\t\t\t\t\t&'
         b'nbsp;&nbsp;<a href="http://gvv.mpi-inf.mpg.de/projects/Vidicontexts/'
         b'" class="button">Project Page</a><br>\r\n\t\t\t\t\tJ. Tompkin, F. P'
         b'ece, R. Shah, S. Izadi, J. Kautz and C. Theobalt<br>\r\n\t\t\t\t\t<'
         b'b>Symposium on User Interface Software and Technology (UIST 2013)</b'
         b'> <br>\r\n\t\t\t\t\tSt Andrews, UK, Oct. 8-11, 2013. <br>\r\n'
         b'\t\t\t\t\t<em><a href="./bibtex/vidicontext_uist2013.bib">Bibtex<'
         b'/a></em><br>\r\n\t\t\t\t\t<em><a href="./videos/vidicontexts.mp4">V'
         b'ideo</a></em><br>\r\n\t\t\t\t\t<em><a class="htooltip">Video previe'
         b'w (on page)\r\n\t\t\t\t\t<span-video>\r\n\t\t\t\t\t\t<video id="my_'
         b'video_1" class="video-js vjs-default-skin" controls\r\n\t\t\t\t\t\t '
         b' preload="auto" width="640" height="360" poster="./videos/vidicontex'
         b'ts.jpg"\r\n\t\t\t\t\t\t  data-setup="{}">\r\n\t\t\t\t\t\t  <source s'
         b'rc="./videos/vidicontexts.mp4" type=\'video/mp4\'>\r\n\t\t\t\t\t\t'
         b'</video>\r\n\t\t\t\t\t</span-video>\r\n\t\t\t\t\t</a></em><br>'
         b'\r\n\t\t\t\t\t<em><a class="htooltip" href="#">Abstract\r\n\t\t'
         b'\t\t\t<span>\r\n\t\t\t\t\t\tVideo collections of places show contra'
         b'sts and changes in our world, but current interfaces to video collec'
         b'tions make it hard for users to explore these changes. \r\n\t\t\t'
         b'\t\t\tRecent state-of-the-art interfaces attempt to solve this prob'
         b"lem for 'outside->in' collections, but cannot connect 'inside->out' "
         b'collections of the same place \r\n\t\t\t\t\t\twhich do not visually '
         b'overlap. We extend the focus+context paradigm to create a video-coll'
         b'ections+context interface by embedding videos into a panorama. \r'
         b'\n\t\t\t\t\t\tWe build a spatio-temporal index and tools for fast e'
         b'xploration of the space and time of the video collection. We demonst'
         b'rate the flexibility of our \r\n\t\t\t\t\t\trepresentation with inte'
         b'rfaces for desktop and mobile flat displays, and for a spherical dis'
         b'play with joypad and tablet controllers. We study with users \r\n\t'
         b'\t\t\t\t\tthe effect of our video-collection+context system to sp'
         b'atio-temporal localization tasks, and find significant improvements '
         b'to accuracy and completion time in \r\n\t\t\t\t\t\tvisual search tas'
         b'ks compared to existing systems. We measure the usability of our int'
         b'erface with System Usability Scale (SUS) and task-specific questionn'
         b'aires, \r\n\t\t\t\t\t\tand find our system scores higher.\r\n\t'
         b'\t\t\t\t</span>\r\n\t\t\t\t\t</a></em>\r\n\t\t\t\t\t</li>\r\n\t\t\t'
         b'\t\t\r\n\t\t\t\t\t<li class="2013 paper">\r\n\t\t\t\t\t<em><a href="'
         b'./pdfs/panoinserts.pdf" class="help_link">PanoInserts: Mobile Spatia'
         b'l Teleconferencing</a></em>\r\n\t\t\t\t\t&nbsp;&nbsp;<a href="http:'
         b'//www.cs.ucl.ac.uk/research/vr/Projects/PanoInserts/" class="button"'
         b'>Project Page</a><br>\r\n\t\t\t\t\tF. Pece, W. Steptoe, S. Julier, '
         b'F. Wanner,T. Weyrich, J. Kautz, and A. Steed <br>\r\n\t\t\t\t\t<b>A'
         b'CM Human Factors in Computing Systems (CHI 2013)</b> <br>\r\n\t'
         b'\t\t\t\tParis, France, April 22-May 2 2013. <br>\r\n\t\t\t\t\t<em><'
         b'font color="red">Paper awarded with a "CHI 2013 Honourable Mention"<'
         b'/font></em><br>\r\n\t\t\t\t\t<em><a href="./bibtex/panoinserts.bib"'
         b'>Bibtex</a></em><br>\r\n\t\t\t\t\t<em><a href="./videos/panoinserts'
         b'.mp4">Video</a></em><br>\r\n\t\t\t\t\t<em><a class="htooltip">Video'
         b' preview (on page)\r\n\t\t\t\t\t<span-video>\r\n\t\t\t\t\t\t<video '
         b'id="my_video_1" class="video-js vjs-default-skin" controls\r\n'
         b'\t\t\t\t\t\t  preload="auto" width="640" height="360" poster="./vi'
         b'deos/panoinserts.jpg"\r\n\t\t\t\t\t\t  data-setup="{}">\r\n\t\t\t'
         b'\t\t\t  <source src="./videos/panoinserts.mp4" type=\'video/mp4\''
         b'>\r\n\t\t\t\t\t\t</video>\r\n\t\t\t\t\t</span-video>\r\n\t\t\t\t\t'
         b'</a></em><br>\r\n\t\t\t\t\t<em><a class="htooltip" href="#">Abstrac'
         b't\r\n\t\t\t\t\t<span>\r\n\t\t\t\t\t\tWe present PanoInserts: a nove'
         b'l teleconferencing system that uses smartphone cameras to create a s'
         b'urround representation\r\n\t\t\t\t\t\tof meeting places. We take a s'
         b'tatic panoramic image of a\r\n\t\t\t\t\t\tlocation into which we ins'
         b'ert live videos from smartphones.\r\n\t\t\t\t\t\tWe use a combinatio'
         b'n of marker- and image-based tracking to\r\n\t\t\t\t\t\tposition the'
         b' video inserts within the panorama, and transmit\r\n\t\t\t\t\t\tthis'
         b' representation to a remote viewer. We conduct a user study\r'
         b'\n\t\t\t\t\t\tcomparing our system with fully-panoramic video and c'
         b'onventional\r\n\t\t\t\t\t\twebcam video conferencing for two spatial'
         b' reasoning\r\n\t\t\t\t\t\ttasks. Results indicate that our system pe'
         b'rforms comparably\r\n\t\t\t\t\t\twith fully-panoramic video, and bet'
         b'ter than webcam video\r\n\t\t\t\t\t\tconferencing in tasks that requ'
         b'ire an accurate surrounding\r\n\t\t\t\t\t\trepresentation of the rem'
         b'ote space. We discuss the representational\r\n\t\t\t\t\t\tproperties'
         b' and usability of varying video presentations,\r\n\t\t\t\t\t\texplor'
         b'ing how they are perceived and how they influence users\r\n\t\t\t'
         b'\t\t\twhen performing spatial reasoning tasks.\r\n\t\t\t\t\t</span'
         b'>\r\n\t\t\t\t\t</a></em>\r\n\t\t\t\t\t</li>\r\n\t\t\t\t\t\r\n\t\t'
         b'\t\t\t<li class="2013 paper">\r\n\t\t\t\t\t<em><a href="./pdfs/BMD'
         b'_JVRC2013.pdf" class="help_link">Bitmap Movement Detection: HDR for '
         b'Dynamic Scenes</a></em><br>\r\n\t\t\t\t\t F. Pece and J. Kautz<br>\r'
         b'\n\t\t\t\t\t<b>Journal of Virtual Reality and Broadcasting</b> <br'
         b'>\r\n\t\t\t\t\t10(2), December 2013, pages 1-13 <br>\r\n\t\t\t\t\t'
         b'Extended CVMP 2010 paper <br>\r\n\t\t\t\t\t<em><a class="htooltip" '
         b'href="#">Abstract\r\n\t\t\t\t\t<span>\r\n\t\t\t\t\t\tExposure Fusio'
         b'n and other HDR techniques generate\r\n\t\t\t\t\t\twell-exposed imag'
         b'es from a bracketed image sequence\r\n\t\t\t\t\t\twhile reproducing '
         b'a large dynamic range that\r\n\t\t\t\t\t\tfar exceeds the dynamic ra'
         b'nge of a single exposure.\r\n\t\t\t\t\t\tCommon to all these techniq'
         b'ues is the problem that the\r\n\t\t\t\t\t\tsmallest movements in the'
         b' captured images generate\r\n\t\t\t\t\t\tartefacts (ghosting) that d'
         b'ramatically affect the quality\r\n\t\t\t\t\t\tof the final images. T'
         b'his limits the use of HDR and\r\n\t\t\t\t\t\tExposure Fusion techniq'
         b'ues because common scenes\r\n\t\t\t\t\t\tof interest are usually dyn'
         b'amic. We present a method\r\n\t\t\t\t\t\tthat adapts Exposure Fusion'
         b', as well as standard HDR\r\n\t\t\t\t\t\ttechniques, to allow for dy'
         b'namic scene without introducing\r\n\t\t\t\t\t\tartefacts. Our method'
         b' detects clusters of moving\r\n\t\t\t\t\t\tpixels within a bracketed'
         b' exposure sequence with\r\n\t\t\t\t\t\tsimple binary operations. We '
         b'show that the proposed\r\n\t\t\t\t\t\ttechnique is able to deal with'
         b' a large amount of movement\r\n\t\t\t\t\t\tin the scene and differen'
         b't movement configurations.\r\n\t\t\t\t\t\tThe result is a ghost-free'
         b' and highly detailed\r\n\t\t\t\t\t\texposure fused image at a low co'
         b'mputational cost.\r\n\t\t\t\t\t</span>\r\n\t\t\t\t\t</a></em>\r'
         b'\n\t\t\t\t\t</li>\r\n\t\t\t\t\t\r\n\t\t\t\t\t</ul>\r\n\t\t\t\t'
         b'\t\r\n\t\t\t\t\t<h3>2012</h3>\r\n\r\n\t\t\t\t\t<ul class="papers"'
         b'>\r\n\r\n\t\t\t\t\t<li class="2012 journal">\r\n\t\t\t\t\t<em><a hre'
         b'f="./pdfs/beaming_ieee.pdf">Beaming: An Asymmetric Telepresence Syst'
         b'em</a></em><br>\r\n\t\t\t\t\tA. Steed, W. Steptoe, W. Oyekoya, F. P'
         b'ece, T. Weyrich, J. Kautz, D. Friedman, A. Peer, M. Solazzi, F. Tecc'
         b'hia, M. Bergamasco, M. Slater<br>\r\n\t\t\t\t\t<b>IEEE Computer Gra'
         b'phics and Applications</b><br>\r\n\t\t\t\t\t32:6, 10-17, 2012.<br>\r'
         b'\n\t\t\t\t\t<em><a class="htooltip" href="#">Abstract\r\n\t\t\t'
         b'\t\t<span>\r\n\t\t\t\t\t\tThe Beaming project recreates, virtually'
         b', a real environment; using immersive VR, remote participants can vi'
         b'sit the virtual model and interact with the people in the real envir'
         b"onment. The real environment doesn't need extensive equipment and ca"
         b'n be a space such as an office or meeting room, domestic environment'
         b', or social space.\r\n\t\t\t\t\t</span>\r\n\t\t\t\t\t</a></em>'
         b'\r\n\t\t\t\t\t</li>\r\n\r\n\t\t\t\t\t<li class="2012 paper">\r\n\t\t'
         b'\t\t\t<em><a href="./pdfs/Simplified_User_Interface_for_Architectur'
         b'al_Reconstruction.pdf">Simplified User Interface for Architectural R'
         b'econstruction</a></em><br>\r\n\t\t\t\t\tF. Wanner, F. Pece, J. Kaut'
         b'z<br>\r\n\t\t\t\t\t<b>Theory and Practice of Computer Graphics 2012'
         b'</b><br>\r\n\t\t\t\t\tRutheford Appleton Lab. - Didcot, UK - 2012<b'
         b'r>\r\n\t\t\t\t\t<em><a class="htooltip" href="#">Abstract\r\n'
         b'\t\t\t\t\t<span>\r\n\t\t\t\t\t\tWe present a user-driven reconstr'
         b'uction system for the creation of 3D models of buildings from photog'
         b'raphs. The\r\n\t\t\t\t\t\tstructural properties of buildings, such a'
         b's parallel and repeated elements, are used to allow the user to crea'
         b'te\r\n\t\t\t\t\t\tefficiently an accurate 3D structure of different '
         b'building types. An intuitive interface guides the user through t'
         b'he\r\n\t\t\t\t\t\treconstruction process, which uses a set of input '
         b'images and a 3D point cloud. The system aims to minimise the\r\n\t\t'
         b'\t\t\t\tuser input by recognising imprecise interaction and ensuring'
         b' photo consistency.\r\n\t\t\t\t\t</span>\r\n\t\t\t\t\t</a></em'
         b">\r\n\t\t\t\t\t</li>\r\n\r\n\t\t\t\t\t<li class='2012 journal'>\r"
         b'\n\t\t\t\t\t<em><a href="./pdfs/actingInMR_presence.pdf">Acting Re'
         b'hearsal in Collaborative Multimodal Mixed Reality Environments</a></'
         b'em><br>\r\n\t\t\t\t\tW. Steptoe, J.-M. Normand, O. Oyekoya, F. Pece'
         b', E. Giannopoulos, F. Tecchia, A. Steed, T. Weyrich, J. Kautz, M. Sl'
         b'ater <br> \r\n\t\t\t\t\t<b>Presence - Teleoperators and Virtual Env'
         b'ironments (2012)</b><br>\r\n\t\t\t\t\t21(4), Fall 2012, pages 406-4'
         b'22<br>\r\n\t\t\t\t\t<em><a href="./bibtex/acting_presence2012.bib">'
         b'Bibtex</a></em><br>\r\n\t\t\t\t\t<em><a class="htooltip" href="#">A'
         b'bstract\r\n\t\t\t\t\t<span>\r\n\t\t\t\t\t\tThis paper presents expe'
         b'rience of using our multimodal mixed reality telecommunication syste'
         b'm\r\n\t\t\t\t\t\tto support remote acting rehearsal. The rehearsals '
         b'involved two actors located in London and\r\n\t\t\t\t\t\tBarcelona, '
         b'and a director in another location in London. This triadic audiovisu'
         b'al\r\n\t\t\t\t\t\ttelecommunication was performed in a spatial and m'
         b'ultimodal collaborative mixed reality\r\n\t\t\t\t\t\tenvironment bas'
         b'ed on the \xe2\x80\x9cdestination-visitor\xe2\x80\x9d paradigm, wh'
         b'ich we define and motivate. We detail\r\n\t\t\t\t\t\tour heterogeneo'
         b'us system architecture, which spans over the three distributed a'
         b'nd\r\n\t\t\t\t\t\ttechnologically-asymmetric sites, and features a r'
         b'ange of capture, display, and transmission\r\n\t\t\t\t\t\ttechnologi'
         b'es. The actors\xe2\x80\x99 and director\xe2\x80\x99s experience of'
         b' rehearsing a scene via the system are then\r\n\t\t\t\t\t\tdiscussed'
         b', exploring successes and failures of this\r\n\t\t\t\t\t</span>'
         b'\r\n\t\t\t\t\t</a></em>\r\n\t\t\t\t\t</li>\r\n\t\t\t\t\t\r\n\t\t\t'
         b'\t\t</ul>\r\n\r\n\t\t\t\t\t<h3>2011</h3>\r\n\r\n\t\t\t\t\t<ul class='
         b'"papers">\r\n\r\n\t\t\t\t\t<li class="2011 conference">\r\n\t\t\t\t'
         b'\t<em><a href="./pdfs/Towards_Moment_Imagery_Automatic_Cinemagrap'
         b'hs.pdf">Towards Moment Imagery: Automatic Cinemagraphs</a></em>\r'
         b'\n\t\t\t\t\t&nbsp;&nbsp;<a href="http://www.cs.ucl.ac.uk/research/'
         b'vr/Projects/AutoCinemagraphs/" class="button">Project Page</a><b'
         b'r>\r\n\t\t\t\t\tJ. Tompkin, F. Pece, K. Subr, J. Kautz<br>\r\n\t\t\t'
         b'\t\t<b>Conference on Visual Media Production (CVMP)</b><br>\r\n\t'
         b'\t\t\t\tLondon, UK, November 2011<br>\r\n\t\t\t\t\t<em><a href="./b'
         b'ibtex/momentImg_cvmp2011.bib">Bibtex</a></em><br>\r\n\t\t\t\t\t<em>'
         b'<a class="htooltip">Video preview (on page)\r\n\t\t\t\t\t<span-vide'
         b'o>\r\n\t\t\t\t\t\t<video id="my_video_1" class="video-js vjs-default'
         b'-skin" controls\r\n\t\t\t\t\t\t  preload="auto" width="640" height="'
         b'370" poster="./videos/autocinemagraphs.jpg"\r\n\t\t\t\t\t\t  data-se'
         b'tup="{}">\r\n\t\t\t\t\t\t  <source src="./videos/autocinemagraphs.mp'
         b'4" type=\'video/mp4\'>\r\n\t\t\t\t\t\t</video>\r\n\t\t\t\t\t</spa'
         b'n-video>\r\n\t\t\t\t\t</a></em><br>\r\n\t\t\t\t\t<em><a class="hto'
         b'oltip" href="#">Abstract\r\n\t\t\t\t\t<span>\r\n\t\t\t\t\t\tThe ima'
         b'gination of the online photographic community\r\n\t\t\t\t\t\thas rec'
         b'ently been sparked by so-called cinemagraphs:\r\n\t\t\t\t\t\tshort, '
         b'seamlessly looping animated GIF images created\r\n\t\t\t\t\t\tfrom v'
         b'ideo in which only parts of the image move. These\r\n\t\t\t\t\t\tcin'
         b'emagraphs capture the dynamics of one particular region\r\n\t\t\t'
         b'\t\t\tin an image for dramatic effect, and provide the creator with'
         b'\r\n\t\t\t\t\t\tcontrol over what part of a moment to capture. We cr'
         b'eate\r\n\t\t\t\t\t\ta cinemagraphs authoring tool combining video mo'
         b'tion\r\n\t\t\t\t\t\tstabilisation, segmentation, interactive motion '
         b'selection, motion\r\n\t\t\t\t\t\tloop detection and selection, and c'
         b'inemagraph rendering.\r\n\t\t\t\t\t\tOur work pushes toward the easy'
         b' and versatile creation\r\n\t\t\t\t\t\tof moments that cannot be rep'
         b'resented with still imagery.\r\n\t\t\t\t\t</span>\r\n\t\t\t\t\t</a'
         b'></em>\t\t\t\t\t\r\n\t\t\t\t\t</li>\r\n\r\n\t\t\t\t\t<li class="2'
         b'011 workshop">\r\n\t\t\t\t\t<em><a href="./pdfs/Three_Depth_Camera_'
         b'Technologies_Compared.pdf">Three Depth-Camera Technologies Compared<'
         b'/a></em><br>\r\n\t\t\t\t\tF. Pece, J. Kautz, T. Weyrich<br>\r\n\t\t'
         b'\t\t\t<b>First BEAMING Workshop</b><br>\r\n\t\t\t\t\tBarcelona, Sp'
         b'ain, 14 June 2011\r\n\t\t\t\t\t</li>\t\r\n\r\n\t\t\t\t\t<li class'
         b'=\'2011 conference\'>\r\n\t\t\t\t\t<em><a href="./pdfs/adapting-s'
         b'tandard-video-codecs-for-depth-streaming.pdf">Adapting Standard Vide'
         b'o Codecs for Depth Streaming</a></em><br>\r\n\t\t\t\t\tF. Pece, J. '
         b'Kautz, T. Weyrich<br>\r\n\t\t\t\t\t<b>Proc. of Joint Virtual Realit'
         b'y Conference of EuroVR (JVRC)</b><br>\r\n\t\t\t\t\tNottingham, UK, '
         b'September 2011<br>\r\n\t\t\t\t\t<em><a href="./bibtex/depthStreamin'
         b'g_jvrc2011.bib">Bibtex</a></em><br>\r\n\t\t\t\t\t<em><a class="htoo'
         b'ltip" href="#">Abstract\r\n\t\t\t\t\t<span>\r\n\t\t\t\t\t\tCameras '
         b'that can acquire a continuous stream of depth images are now commonl'
         b'y available, for instance the\r\n\t\t\t\t\t\tMicrosoft Kinect. It ma'
         b'y seem that one should be able to stream these depth videos using st'
         b'andard video codecs,\r\n\t\t\t\t\t\tsuch as VP8 or H.264. However, t'
         b'he quality degrades considerably as the compression algorithms are g'
         b'eared\r\n\t\t\t\t\t\ttowards standard three-channel (8-bit) colour v'
         b'ideo, whereas depth videos are single-channel but have a higher\r'
         b'\n\t\t\t\t\t\tbit depth. We present a novel encoding scheme that ef'
         b'ficiently converts the single-channel depth images to standard\r\n'
         b'\t\t\t\t\t\t8-bit three-channel images, which can then be streamed'
         b' using standard codecs. Our encoding scheme ensures that\r\n\t\t'
         b'\t\t\t\tthe compression affects the depth values as little as possib'
         b'le. We show results obtained using two common video\r\n\t\t\t\t\t\te'
         b'ncoders (VP8 and H.264) as well as the results obtained when using J'
         b'PEG compression. The results indicate that\r\n\t\t\t\t\t\tour encodi'
         b'ng scheme performs much better than simpler methods.\r\n\t\t\t\t\t<'
         b'/span>\r\n\t\t\t\t\t</a></em>\r\n\t\t\t\t\t</li>\t\r\n\r\n\t\t\t\t\t'
         b'</ul>\r\n\r\n\t\t\t\t\t<h3>2010</h3>\r\n\r\n\t\t\t\t\t<ul class="p'
         b'apers">\r\n\r\n\t\t\t\t\t<li class=\'2010 conference\'>\r\n\t\t'
         b'\t\t\t<em><a href="./pdfs/bmd_cvmp.pdf">Bitmap Movement Detection: '
         b'HDR for Dynamic Scenes</a></em><br>\r\n\t\t\t\t\tF. Pece, J. Kautz<'
         b'br>\r\n\t\t\t\t\t<b>Conference on Visual Media Production (CVMP)</b'
         b'><br>\r\n\t\t\t\t\tLondon, UK, November 2010<br>\r\n\t\t\t\t\t<em>'
         b'<a href="./bibtex/bmd_cvmp2010.bib">Bibtex</a></em><br>\r\n\t\t\t'
         b'\t\t<em><a class="htooltip" href="#">Abstract\r\n\t\t\t\t\t<span>'
         b'\r\n\t\t\t\t\t\tExposure Fusion and other HDR techniques generate we'
         b'll-exposed\r\n\t\t\t\t\t\timages from a bracketed image sequence whi'
         b'le\r\n\t\t\t\t\t\treproducing a large dynamic range that far exceeds'
         b' the\r\n\t\t\t\t\t\tdynamic range of a single exposure. Common to al'
         b'l these\r\n\t\t\t\t\t\ttechniques is the problem that the smallest m'
         b'ovements\r\n\t\t\t\t\t\tin the captured images generate artefacts (g'
         b'hosting) that\r\n\t\t\t\t\t\tdramatically affect the quality of the '
         b'final images. This limits\r\n\t\t\t\t\t\tthe use of HDR and Exposure'
         b' Fusion techniques because\r\n\t\t\t\t\t\tcommon scenes of interest '
         b'are usually dynamic. We present a\r\n\t\t\t\t\t\tmethod that adapts '
         b'Exposure Fusion, as well as standard HDR\r\n\t\t\t\t\t\ttechniques, '
         b'to allow for dynamic scene without introducing\r\n\t\t\t\t\t\tartefa'
         b'cts. Our method detects clusters of moving pixels within\r\n\t\t'
         b'\t\t\t\ta bracketed exposure sequence with simple binary operations.'
         b'\r\n\t\t\t\t\t\tWe show that the proposed technique is able to deal '
         b'with a\r\n\t\t\t\t\t\tlarge amount of movement in the scene and diff'
         b'erent movement\r\n\t\t\t\t\t\tconfigurations. The result is a ghost-'
         b'free and highly detailed\r\n\t\t\t\t\t\texposure fused image at a lo'
         b'w computational cost.\r\n\t\t\t\t\t</span>\r\n\t\t\t\t\t</a></em><'
         b'br>\r\n\t\t\t\t\t<em><a href="./files/bmd_dataset.zip">Dataset</a><'
         b'/em><br>\r\n\t\t\t\t\t</li>\r\n\t\t\t\t\t\r\n\t\t\t\t\t</ul>\r'
         b'\n\r\n\t\t\t\t\t<h3>2009</h3>\r\n\r\n\t\t\t\t\t<ul class="papers"'
         b">\r\n\r\n\t\t\t\t\t<li class='2009 thesis'>\r\n\t\t\t\t\t<em><a href"
         b'="./pdfs/Final_Fabrizio.pdf">High Dynamic Range for Dynamic Scenes</'
         b'a></em><br>\r\n\t\t\t\t\tFabrizio Pece<br>\r\n\t\t\t\t\tMSc VIVE T'
         b'hesis<br>\r\n\t\t\t\t\tSeptember 2009\r\n\t\t\t\t\t</li>\r\n\r\n\t\t'
         b'\t\t\t</ul>\r\n\r\n\t\t\t\t\t<h3>2008</h3>\r\n\r\n\t\t\t\t\t<ul c'
         b'lass="papers">\r\n\r\n\t\t\t\t\t<li class=\'2008 thesis\'>\r\n\t\t\t'
         b'\t\t<em><a href="./pdfs/tesi_FabrizioPece.pdf">Efficient Represent'
         b'ation for Image Classification Based on Support Vector Machines</a><'
         b'/em><br>\r\n\t\t\t\t\tFabrizio Pece<br>\r\n\t\t\t\t\tBSc Informati'
         b'cs Engineering Thesis<br>\r\n\t\t\t\t\t</li>\r\n\r\n\t\t\t\t\t</'
         b'ul>\r\n\t\t\t\t\t\r\n\t\t\t\t</div>\r\n\t\t\t\t\r\n\t\t\t\t<h2>Pa'
         b"tents and Others <span class='group'></span></h2>\t\r\n\t\t\t\t<div"
         b' class="papers">\r\n\t\t\t\t\r\n\t\t\t\t\t<ul class="papers">'
         b'\r\n\r\n\t\t\t\t\t<li class="2013 paper">\r\n\t\t\t\t\t<em><a href="'
         b'./pdfs/Physical reproduction of reflectance fields.pdf" class="help_'
         b'link">Physical reproduction of reflectance fields</a></em>\r\n'
         b'\t\t\t\t\t&nbsp;&nbsp;<a href="http://www.google.com/patents/US20'
         b'130016100#classifications" class="button">Patent Page</a><br>\r\n\t'
         b'\t\t\t\tBickel, B. and Alexa, M. and Kautz, J. and Matusik, W. and P'
         b'ece, F.<br>\r\n\t\t\t\t\t<b>US Patent App. 13/608,819</b> <br>\r'
         b'\n\t\t\t\t\tFilled date\t10/09/2012 <br>\r\n\t\t\t\t\tPublication '
         b'date 17/01/2013 <br>\r\n\t\t\t\t\t<em><a href="./bibtex/disney_pate'
         b'nt.bib">Bibtex</a></em><br>\r\n\t\t\t\t\t</li>\r\n\t\t\t\t\t\r\n'
         b'\t\t\t\t\t</ul>\r\n\t\t\t\t\r\n\t\t\t\t</div>\r\n\t\t\t</div>\r'
         b'\n\t\t</div>\r\n\t</div>\r\n\t\r\n\t<div id="footercontainer">\r\n'
         b'\t\t<footer class="group">\r\n\t\t\t\t\t<div class="col span_2_of'
         b'_4">\r\n\t\t\t\t\t\t<h4>Bio</h4>\r\n\t\t\t\t\t\t\t<p>\tI currently'
         b' work as a Researcher at <a href="https://www.ethz.ch/en.html" targe'
         b't="_blank">ETH Zurich</a>, \r\n\t\t\t\t\t\t\t\twith <a href="http:'
         b'//ait.inf.ethz.ch/people/hilliges/" target="_blank">Professor Otmar '
         b'Hilliges</a> \r\n\t\t\t\t\t\t\t\tin the <a href="http://ait.inf.et'
         b'hz.ch/" target="_blank">Advanced Interactive Technologies lab</a'
         b'>.\r\n\t\t\t\t\t\t\t\tYou can find my new webpage <a href="http://'
         b'ait.inf.ethz.ch/people/pece/" target="_blank">here</a>. <br>\r\n\t\t'
         b'\t\t\t\t\t\tPrior to joining ETH I was a PhD student in the Virtua'
         b'l Environment and Computer Graphics group in UCL-CS, \r\n\t\t\t\t\t'
         b'\t\t\tsupervised by <a href="http://www.cs.ucl.ac.uk/staff/j.kautz/'
         b'" target="_blank">Prof. Jan Kautz</a>.<br>\r\n\t\t\t\t\t\t\t\tMy <'
         b'a href="./research/index.html">research interests</a> are virtual, m'
         b'ixed and augmented realities, telepresence, videos in panoramic cont'
         b'ext, \r\n\t\t\t\t\t\t\t\tpanoramic imagery and human-computer inte'
         b'raction. I am also interested in mobile interaction, ubiquitous comp'
         b'uting, 3D reconstruction from videos and images, \r\n\t\t\t\t\t'
         b'\t\t\tHDR and time-varying imagery. \r\n\t\t\t\t\t\t\t</p>\t\r'
         b'\n\t\t\t\t\t</div>\r\n\t\t\t\r\n\t\t\t<div class="col span_1_of_4"><'
         b'/div>\r\n\t\t\t\r\n\t\t\t<div class="col span_1_of_4">\r\n\t\t\t\t<h'
         b'4>Contacts</h4>\r\n\t\t\t\t<p>\t<address><script type="text/javascr'
         b'ipt">hiddenemail(\'inf.ethz.ch\',\'fabrizio.pece\')</script><br>'
         b'\r\n                    Universtitatstrasse 6 <br>\r\n              '
         b'      8092 Zurich, Switzerland <br>\r\n\t\t\t\t\tETH Zurich, CS Dep'
         b't.<br>\r\n                    <a href="http://www.mapsearch.ethz.c'
         b'h/map/mapSearchPre.do?farbcode=c010&lang=EN&raumMap=56&gebaeudeMap=C'
         b'NB"> CNB</a>, \r\n\t\t\t\t\t<a href="http://www.rauminfo.ethz.ch/Ra'
         b'uminfo/grundrissplan.gif?gebaeude=CNB&geschoss=H&raumNr=100.9&lang=e'
         b'n">H 100.9 </a><br>\r\n\t\t\t\t\tP: +41 (0)44 632 07 68<br>\r\n\t\t'
         b'\t\t\tF: +41 (0)44 632 16 59<br></p>\r\n\t\t\t\t<a href="https://'
         b'twitter.com/fabriziopece" class="twitter-follow-button" data-show-co'
         b'unt="false">Follow @fabriziopece</a>\r\n\t\t\t\t<script>!function('
         b'd,s,id){var js,fjs=d.getElementsByTagName(s)[0];if(!d.getElementById'
         b'(id)){js=d.createElement(s);js.id=id;js.src="//platform.twitter.com/'
         b'widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script"'
         b',"twitter-wjs");</script>\r\n\t\t\t</div>\r\n\r\n\r\n    \t\t<br '
         b'class="breaker" />\r\n\r\n\t\t\t<div id="smallprint">\r\n\r\n\t\t'
         b'\t<a href="http://validator.w3.org/"><img src="../images/html5-lo'
         b'go.png" width="40" height="50" alt="html5" /></a>\r\n\t\t\t<a hre'
         b'f="http://jigsaw.w3.org/css-validator/"><img src="../images/css3-log'
         b'o.png" width="40" height="50" alt="css3" /></a>\r\n\r\n\t\t\t<br />'
         b'\r\n\r\n\t\t\t<p>&copy; Copyright <a href="http://www.cs.ucl.ac.uk/'
         b'staff/F.Pece/">Fabrizio Pece</a> 2014. Built with the <a href="http:'
         b'//www.responsivegridsystem.com">Responsive Grid System</a> - Webpage'
         b' design inspired by <a href="http://www.grahamrobertsonmiller.co.uk/'
         b'"> Graham Miller</a>.<br />\r\n\t\t\t<p>\t</p>\r\n\t\t\t</div>\r'
         b'\n\t\t</footer>\r\n\t</div>\r\n</div>\r\n\r\n\t<!-- JavaScript at'
         b' the bottom for fast page loading -->\r\n\r\n\t<!-- Grab Google C'
         b"DN's jQuery, with a protocol relative URL; fall back to local if nec"
         b'essary -->\r\n\t<script src="//ajax.googleapis.com/ajax/libs/jquery'
         b'/1.7.2/jquery.min.js"></script>\r\n\t<script>window.jQuery || docum'
         b'ent.write(\'<script src="./js/jquery-1.7.2.min.js"><\\/script>\')</'
         b'script>\r\n\t<!-- More Scripts-->\r\n\t<script src="./../js/supers'
         b'leight/supersleight.plugin.js"></script>\r\n\t<script src="./../js/'
         b'hoverIntent-r6.js"></script>\r\n\t<script src="./../js/plugins.js">'
         b'</script>\r\n\t<script src="./../js/helper.js"></script>\r\n\t<scr'
         b'ipt src="./../js/fabriziopece.js"></script>\r\n\t<script src="./../'
         b'js/hiddenemail.js"></script>\r\n\r\n\r\n</body>\r\n</html>\r\n',
 'links': [2682,
           2681,
           2709,
           2710,
           2711,
           2712,
           2713,
           2714,
           2715,
           2716,
           2717,
           2685,
           2718,
           1690],
 'pid': 2682,
 'url': 'http://www0.cs.ucl.ac.uk/staff/F.Pece/publications/index.html'}