{'html': b"<HTML>\n<TITLE>Mark Herbster's Publications</TITLE>\n<body bgcolor"
         b'="#FFFFFF" link="#0000FF" vlink="#0000FF">\n<!-- <H2>Working Pape'
         b'rs:</H2> -->\n\n<H2>\nPublications:</H2>\n<A href="cf-star-2-full.pd'
         b'f">\nMistake Bounds for Binary Matrix Completion,</A>&nbsp  \nM. H'
         b'erbster, S. Pasteris, M. Pontil, NIPS 2016. &nbsp \n<p>\n\n<A href='
         b'\n"cold-20-long.pdf">\nOnline Prediction at the Limit of Zero Temp'
         b'erature</A>,&nbsp  \nM. Herbster, S. Pasteris, S. Ghosh, NIPS 201'
         b'5. &nbsp \n<p>\n\n\n<A href=\n"http://www.jmlr.org/papers/volume1'
         b'6/herbster15a/herbster15a.pdf">Predicting\na Switching Sequence o'
         b'f Graph Labelings,<A> M. Herbster, S. Pasteris,\nM. Pontil, Journ'
         b'al of Machine Learning Research pp 2003-2022,\nSeptember 2015  &n'
         b'bsp \n<p>\n\n<A href="ghp13v2.pdf">Online Similarity Prediction of '
         b'Networked\nData \nfrom Known and Unknown Graphs</A>,&nbsp  \nC. Gen'
         b'tile, M. Herbster, S. Pasteris, COLT 2013.&nbsp <A href="ghp13v2.pdf'
         b'">[paper]</A>\n<p>\n\n\n\n<A href="OSP-short-4.pdf">Online Sum-Pr'
         b'oduct Computation over Trees</A>,&nbsp  \nM. Herbster, S. Pasteri'
         b's, F. Vitale, NIPS 2012.&nbsp <A href="OSP-short-4.pdf">[paper]<'
         b'/A>\n<p>\n\n\n<A HREF="mc14.pdf"> Efficient Prediction for Tree Mark'
         b'ov Random\nFields in a Streaming Model</A>, \nM.Herbster, S. Paste'
         b'ris, F. Vitale, NIPS Workshop on Discrete Optimization in Machine Le'
         b'arning (DISCML)\n2011: Uncertainty, Generalization and Feedback. '
         b' &nbsp <A\nHREF="mc14.pdf">[paper]</A>&nbsp\n<p>\n\n\n<A HREF="tr'
         b'iangle.pdf"> A Triangle Inequality for p-Resistance </A>, M. Herbste'
         b'r, Networks Across Disciplines: Theory and\nApplications : Worksh'
         b'op @ NIPS 2010.  &nbsp <A\nHREF="triangle.pdf">[paper]</A>&nbsp\n<'
         b'p>\n<A HREF="pgraph.pdf"> Predicting the Labelling of a Graph via'
         b' Minimum p-Seminorm\nInterpolation</A>, M. Herbster and G. Lever,'
         b' COLT 2009.  &nbsp <A\nHREF="pgraph.pdf">[paper]</A>&nbsp <A\nHREF'
         b'="hpmin.pdf">[slides]</A><sup><a href="#sparse" id="fn1">1</a></sup>'
         b'\n<p>\n<A HREF="fptnipsfin.pdf">Fast Prediction on a Tree</A>, M. '
         b'Herbster, M. Pontil, S. Rojas Galeano,\nNIPS 22, 2008. &nbsp <A H'
         b'REF="fptnipsfin.pdf">[paper]</A>&nbsp <A HREF="handfptnips08.pdf">[s'
         b'lides]</A>\n<p>\n<A HREF="spinenipsfin.pdf">Online Prediction on L'
         b'arge Diameter Graphs</A>, M. Herbster, G. Lever, and\nM. Pontil, '
         b'NIPS 22, 2008. &nbsp <A HREF="spinenipsfin.pdf">[paper]</A>&nbsp <A '
         b'HREF="spinespotlight">[1-slide]</A>\n<p>\n<A HREF="postcomprox.pdf'
         b'"> Exploiting cluster-structure to predict the\nlabeling of a gra'
         b'ph, </A> M.Herbster, Proceedings of The 19th International Conferenc'
         b'e\non Algorithmic Learning Theory (ALT\'08), 2008. &nbsp <A\nHREF="'
         b'postcomprox.pdf">[paper]</A>&nbsp <A HREF="printaltproximalmintalk.p'
         b'df">[slides]</A>\n<p>\n<A HREF="perclowerrn.pdf"> A Linear Lower B'
         b'ound for the Perceptron for\nInput Sets of Constant Cardinality</'
         b'A>, M. Herbster, Research Note,  Dept. of Computer\nScience, UCL,'
         b' March 2008. (Updated 18 May 08)\n<p>\n<A HREF="subecmltree.pdf"> '
         b'A fast method to predict the labeling of a\ntree </A> S.R. Galean'
         b'o, and M.Herbster, Graph\nLabeling Workshop (ECML-2007), 2007.\n<p'
         b'>\n<A HREF="boundgraph.pdf">\nPrediction on a graph with a percept'
         b'ron\n</A>M. Herbster, and M. Pontil, NIPS 20, 2006. &nbsp\n<P><A H'
         b'REF="http://www.cs.ucl.ac.uk/staff/M.Pontil/reading/graph-learning.p'
         b'df">\nCombining graph laplacians for semi--supervised learning\n</'
         b'A> A. Argyriou, M. Herbster, and M. Pontil, NIPS 19, 2005. &nbsp'
         b'\n<P> <A\nHREF="gpercicmlfin.pdf">Online learning over\ngraphs</A> '
         b'M. Herbster, M. Pontil, and L. Wainer, Proc. 22nd\nInt. Conf. Mac'
         b'hine Learning (ICML\'05), 2005.&nbsp <A\nHREF="gpercicmlfin.pdf">['
         b'paper]</A>&nbsp <A HREF="graphtalkicmlaug05.pdf">[slides]</A>\n\n<'
         b'P><A HREF="alt_fin_04_herbsterb.pdf">Relative Loss Bounds and Polyno'
         b'mial-time Predictions for\nthe K-LMS-NET Algorithm</A>&nbsp M. He'
         b'rbster, Proceedings of The 15th International Conference\non Algo'
         b'rithmic Learning Theory, October 2004.</B>\n<BLOCKQUOTE>  An onli'
         b'ne algorithm is given whose hypothesis class is a union\nof param'
         b'eterized kernel spaces, for example the set of spaces induced by'
         b'\nGaussian kernel when the width is varied.  We give relative los'
         b's\nbounds and a tractable algorithm for specific kernels.\n</block'
         b'quote>\n\n<P><A HREF="barc_iperp_fin.pdf">Relative loss bounds for'
         b'\npredicting almost as well as any function in a union of Gaussia'
         b'n\nreproducing kernel spaces with varying widths</A>&nbsp M. Herb'
         b'ster,\nPoster at Mathematical Foundations Learning Theory, June 2'
         b'004\n</B>\n\n<P><A\nHREF="http://www.ai.mit.edu/projects/jmlr/papers'
         b'/volume1/herbster01a/herbster01a.ps">\nTracking\nthe best linear p'
         b'redictor</A> Mark Herbster and Manfred Warmuth, Journal\nof Machi'
         b'ne Learning Research pp 281-309, September 2001.\n</B>\n<BLOCKQUOT'
         b'E>\nWe extend the results of "Tracking the best expert" (see belo'
         b'w) to\nlinear combinations.\n<BR></BLOCKQUOTE>\n\n<P><A HREF="wayfas'
         b'ter.pdf"> Learning additive models online with fast evaluating kerne'
         b'ls</A>\nMark Herbster, An abstract\nappeared in Proceedings 14th A'
         b'nnual Conference on Computational\nLearning Theory pp 444-460, Ju'
         b'ly 2001.</B>\n<p>\n\n\n<!--\n<P><B><A HREF="http://www.cse.ucsc.e'
         b'du/~mark/papers/mjhthesis.ps">"Learning&nbsp;\nalgorithms for tra'
         b'cking concepts and an investigation into the error surfaces\nof s'
         b'ingle artificial neurons"</A> Mark Herbster, PhD Thesis, December 19'
         b'98.&nbsp;</B>\n<BLOCKQUOTE>This thesis is an extension of the res'
         b'ults in the following\nthree papers.&nbsp; In particular the resu'
         b'lts of "Tracking the Best Regressor"\nare significantly extended.'
         b'\n<BR>&nbsp;</BLOCKQUOTE>\n-->\n\n<!--\n<A HREF="http://www.cse.u'
         b'csc.edu/~mark/papers/trackregred.ps">Tracking\nthe best regressor'
         b'</A> Mark Herbster and Manfred Warmuth,&nbsp; An abstract\nappear'
         b'ed in Proc. 12th Annu. Conf. on Comput. Learning Theory pp. 24-3'
         b'1,\nJuly 1998.</B>\n<BR>&nbsp;\n<BLOCKQUOTE>\nThis is the conference'
         b' version of "Tracking the best linear predictor."  \n</BLOCKQUOTE'
         b'>\n&nbsp;\n-->\n<DT>\n<A HREF="exp-min.pdf">Exponentially\nmany l'
         b'ocal minima for single neurons</A> Peter Auer, Mark Herbster and'
         b'\nManfred Warmuth, Neural Information Processing Systems 1996</B>'
         b'</DT>\n\n<BLOCKQUOTE>We show that for a single neuron with the log'
         b'istic function\nas the transfer function the number of local mini'
         b'ma of the error function\nbased on the square loss can grow expon'
         b'entially in the dimension.</BLOCKQUOTE>\n\n<DT>\n<A HREF="track-lon'
         b'g.pdf">Tracking\nthe best expert [Long Version]</A> Mark Herbster'
         b' and Manfred Warmuth,\nMachine Learning, Aug. 1998, vol.32, (no.2'
         b'):151-78</B></DT>\n\n<BLOCKQUOTE>We generalize the recent worst-ca'
         b'se loss bounds for on-line\nalgorithms where the additional loss '
         b'of the algorithm on the whole sequence\nof examples over the loss'
         b' of the best expert is bounded. The generalization\nallows the se'
         b'quence to be partitioned into segments and the goal is to\nbound '
         b'the additional loss of the algorithm over the sum of the losses '
         b'of\nthe best experts of each segment. This is to model situations'
         b' in which\nthe examples change and different experts are best for'
         b' certain segments\nof the sequence of examples. In the single exp'
         b'ert case the additional loss\nis proportional to $\\log n$, where '
         b'$n$ is the number of experts and the\nconstant of proportionality'
         b' depends on the loss function. When the number\nof segments is at'
         b' most $k+1$ and the sequence of length $\\ell$ then we\ncan bound '
         b'the additional loss of our algorithm over the best partitioning\n'
         b'by $O(k \\log n + k \\log(\\ell/k))$. Note that it takes the same o'
         b'rder of\nbits to denote the sequence of experts and the boundarie'
         b's of the segments.\nWhen the loss per trial is bounded by one the'
         b'n we obtain additional loss\nbounds that are independent of the l'
         b'ength of the sequence. The bound becomes\n$O(k\\log n+ k \\log(L/k)'
         b')$, where $L$ is the loss of the best partition\ninto $k+1$ segme'
         b'nts. Our algorithms for tracking the best expert are simple\nadap'
         b"tations of Vovk's original algorithm for the single best expert case"
         b'.\nThese algorithms keep one weight per expert and spend $O(1)$ t'
         b'ime per weight\nin each trial.</BLOCKQUOTE>\n\n<DT>\n<A HREF="gibbs.'
         b'pdf">RNA Modeling\nUsing Gibbs Sampling and Stochastic Context Fr'
         b'ee Grammars</A>, Leslie\nGrate, Mark Herbster, Richard Hughey, Da'
         b'vid Haussler I. Saira Mian, and\nHarry Noller, Proceedings of Int'
         b'elligent Systems in Molecular Biology 1994</B></DT>\n\n<BLOCKQUOTE'
         b'>A new method of discovering the common secondary structure\nof a'
         b' family of homologous RNA sequences using Gibbs sampling and stochas'
         b'tic\ncontext-free grammars is proposed. Given an unaligned set of'
         b' sequences,\na Gibbs sampling step simultaneously estimates the s'
         b'econdary structure\nof each sequence and a set of statistical par'
         b'ameters describing the common\nsecondary structure of the set as '
         b'a whole. These parameters describe a\nstatistical model of the fa'
         b'mily. After the Gibbs sampling has produced\na crude statistical '
         b'model for the family, this model is translated into\na stochastic'
         b' context-free grammar, which is then refined by an Expectation\nM'
         b'aximization (EM) procedure to produce a more complete model. A proto'
         b'type\nimplementation of the method is tested on tRNA, pieces of 1'
         b'6S rRNA and\non U5 snRNA with good results.</BLOCKQUOTE>\n<hr>\n<di'
         b'v id="footnote">\n<ol>\n<li id="sparse"> Sparsity in Machine Learn'
         b'ing and Statistics Workshop @\nCumberland Lodge, April 1-3, 2009"'
         b' <a href="#fn1">&uarr;</a></li>\n</ol>\n</div>\n\n<HR>\n<ADDRESS>'
         b'\nmaintained by Mark Herbster / <A HREF="mailto:M.Herbster@cs.ucl'
         b'.ac.uk">M.Herbster@cs.ucl.ac.uk</A></ADDRESS>\n</HTML>\n\n\n\n\n\n\n'
         b'\n\n\n\n\n\n',
 'links': [3462, 3517],
 'pid': 3462,
 'url': 'http://www0.cs.ucl.ac.uk/staff/M.Herbster/pubs/index.html'}