{'html': '<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">\n<HTML><HEAD><TITLE>GI01/M055: Supervised Learning, Fall 2014</TITLE>\n<META NAME="ROBOTS" CONTENT="NOINDEX,NOFOLLOW"> \n<META http-equiv=Content-Type content="text/html; charset=windows-1252">\n<META content="MSHTML 6.00.2900.2963" name=GENERATOR></HEAD>\n<BODY text=#000000 bgColor=#ffffff>\n<H1>\n<CENTER>GI01/M055: Supervised Learning, Fall 2014</CENTER></H1>\n<HR>\n\n\n  <TABLE>\n    <TBODY>\n    <TR>\n      <TD align=right><B>Class Times:</B></TD>\n      <TD>Mondays, 14:00--17:00 </TD></TR>\n    <TR>\n      <TD align=right><B>Location:</B></TD>\n      <TD>Roberts 4.21</TD></TR>\n    <TR>\n      <TD align=right><B>Instructor (first half):</B></TD>\n      <TD><A href="http://www.cs.ucl.ac.uk/staff/M.Herbster/">Mark Herbster</A> Office: 8.03, CS Building, Malet Place </TD></TR>\n <TR>\n      <TD align=right><B>Instructor (second half):</B></TD>\n      <TD><A href="http://www.cs.ucl.ac.uk/staff/M.Pontil/">Massimiliano Pontil</A> Office: 8.08, CS Building, Malet Place </TD></TR> \n    <TR><!--         <td align="right"><b>Office Hours:</b></td>\n         <td></td>--></TR>\n    <!--<TR>\n      <TD align=right><B>Email Contacts :</B></TD>\n      <TD><B><A href="mailto:m.herbster@cs.ucl.ac.uk">m.herbster@cs.ucl.ac.uk</A></B></TD>\n      <TD><B><A href="mailto:m.pontil@cs.ucl.ac.uk">m.pontil@cs.ucl.ac.uk</A></B></TD></TR>\n      -->\n      </TBODY></TABLE>\n  <UL>\n\n <H2><A name=description></A><B>Course description</B></H2>  The course covers\n  supervised approaches to machine learning. It starts by probabilistic pattern recognition followed by an\n  in-depth introduction to various supervised learning algorithms such as Least\n  Squares, Lasso, Perceptron Algorithm, Support Vector Machines and Boosting.\n  <H2><A name=requirements></A><B>Prerequisites</B> </H2>Calculus, basic\n  probability, basic linear algebra.\n  <H2><A name=grading></A><B>Grading</B> </H2>The course has the following\n  assessment components: 1) Written Examination (2.5 hours, 75%) , 2) Coursework\n  Section (2 pieces, 25%). To pass this course, students must obtain an average of at least 50% when the\n  coursework and exam components of a course are weighted together.\n  <P></P>\n  <H2><A name=psets></A><B>Problem sets (Two - to be announced) </B> </H2>\n\n <A href="lab1session14v2.pdf"> Coursework #1 (lab 1):</A> (Due 16 January\n  2015) <BR>\n<!--\n<A href="homework-massi-2013.pdf"> Coursework #2:</A> (Due 2 December\n  2013)\n-->\n<!-- -->\n<!--\n<BR>  <A href="lab1session11v2.pdf"> Lab 1/Coursework  #2:</A> (Due 22 November 2010)  <BR>  \n Coursework #3: <A href="lab2"> Lab 2</A> <BR> --> \n <P></P>\n  <H2><A name=briefsyllabus></A>Syllabus </H2>The schedule of the course is\n  listed below. Follow the link for each class to find lecture slides.\n  <P><BR>\n  <TABLE border=1>\n    <TBODY>\n    <TR>\n      <TH>Date</TH>\n      <TH>Title</TH></TR>\n    <TR>\n      <TD>Monday, September 29</TD>\n      <TD><A\n        href="SLIntro-14.pdf">Introduction\n        to Supervised Learning</A></TD></TR>\n    <TR>\n            <TD>Monday, October 6</TD>\n     <TD><A href="13-5-class.pdf">Statistical Learning Theory (DRAFT) </A></TD></TR>\n<!-- <TD>Statistical Learning Theory <TR> -->\n\n<TD>Monday, October 13</TD>\n      <TD><A href="SL-kernreg14">Kernels and Regularisation (DRAFT)</A></TD></TR>\n<!--      <TD>Kernels and Regularisation</TD></TR> --> \n     <TR>\n      <TD>Monday, October 20 </TD>\n<!--      <TD><A href="lab1session13v4.pdf"></A></TD></TR> -->\n      <TD>Lab: Applying Regresion </TD></TR>\n    <TR>\n      <TD>Monday, October 27</TD>\n<!--      <TD>Support Vector Machines</TD></TR> -->\n      <TD><A href="svm.pdf">Support vector machines (DRAFT) </A></TD></TR>          </TD></TR>\n    <TR>\n            <TD>Monday, November 3</TD>\n      <TD>No lectures (reading week)</A></TD></TR>\n    <TR>\n      <TD>Monday, November 10</TD>\n<!-- <TD>Sparsity Methods</TD> -->\n<TD><A href="SL-ssl-14v1.pdf">Graph-based Semi-Supervised Learning</A></TD>\n      </TR>\n<TR><TD>Monday, November 17</TD>\n<TD><A href="lec-proximalmethods.pdf">Proximal Methods (DRAFT) </A></TD>\n<!-- <TD>Proximal Methods</TD> -->\n        </TR>   \n    <TR>\n            <TD>Monday, November 24</TD>\n<TD><A href="mt_class.pdf">Multi-task Learning (DRAFT)</A></TD>\n<!-- <TD>Multi-task Learning</A> -->\n\t    </TD>      \n\n      </TR>\n     <TR> <TD>Monday, December 1</TD>\n<TD><A href="13-lec-sparsemodels.pdf">Sparsity-based Methods</A></TD>\n</TR>\n     <TR> <TD>Monday, December 8</TD>\n<TD><A href="SL-onlinelearning14v2.pdf">Online Learning (DRAFT)</A></TD>\n        </TR>\n<TR>\n    </TBODY></TABLE></P>\n  <H2><A name=readings></A><B>Reading list</B> </H2>\n  <B>Main reference:</B><p>\n    <UL>    \n<LI>T. Hastie, R. Tibshirani and J. Friedman. <B>The Elements of Statistical\n    Learning: Data Mining, Inference, and Prediction. </B>Springer, 2002.\n</UL>\n    <P><B>Other suggested references:</B> </P>\n<UL>    \n<LI>C.M. Bishop. <B>\n Pattern Recognition and Machine Learning\n </B> Springer, 2006.\n  <LI> N. Cristianini and J. Shawe-Taylor. <B> An Introduction to Support Vector Machines </B> Cambridge University Press, 2001.\n    <LI>R.O. Duda, P.E. Hart and D.G. Stork. <B>Pattern Classification.</B>\n    Wiley, 2nd edition, 2004.\n    <LI>D.J.C. MacKay. <B>Information Theory, Pattern Recognition and Neural\n    Networks.</B> Cambridge Press, 2003\n    <LI>T. Mitchell. <B>Machine Learning.</B> McGraw Hill, 1997\n    <LI>J. Shawe-Taylor and N. Cristianini. <B>Kernel Methods for Pattern\n    Analysis. </B> Cambridge University Press, 2004.\n    <LI>B.Scholkopf and A.J. Smola. <B>Learning with Kernels</B>. MIT Press,\n    2002.\n    <LI>V.N. Vapnik. <B>Statistical Learning Theory</B>. Wiley, New York, 1998.\n    </LI>\n</UL>\n\n<HR>\n</BODY></HTML>\n',
 'pid': 155,
 'url': 'http://www0.cs.ucl.ac.uk/staff/M.Herbster/GI01/'}